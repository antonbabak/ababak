Word2vec embedding #2
Проект позволяет создавать векторное представление слов на основании корпуса текстов. Отличия от известных вариантов реализаций заключается в алгоритме тренировки. Корпус текстов разбивается на предложения. Предложения на токены. Затем для каждого токена вычисляется список окружающих его слов. Декларируется стандартная сеть с одним скрытым слоем. При тренировки сети на вход подается минибатч onehot векторов с единицей в индексе соответствующем слову. Дальше для каждого слова происходит вычисление граунд тру вектора (размер равен числу токенов) с вероятностями встречи входного слова (выходной вектор вычисляется на основании ранее составленного словаря со списками). После чего скалярно перемножатюся выход сети и граунд тру вектор и от произведения считается ошибка.


100 мерный вектор начинает иметь какой-то смысл. После 15 эпох получились следующие примеры соседних векторов:


Химия

15297                   неживые
16726                  знакомые
21312           многоотраслевая
21315    материальнотехническая
21860                   журфака
22030                сирийского
24829                   потерях
25179              колониальным
27496                 подавлять
28191               каролинском
31541                    полета
33568            систематически
36817                    горное
38028             израильтянами
39005                    цюриха
40010         послеоперационных
41271                   колонна


культура
359            наиболее
475        реабилитация
771                лицо
1418            функции
2088           личности
2099             сестра
2174              наука
3633            каждого
3812         управление
4407               вера
7454             совета
11229    санктпетербург
15761           выпуска
19456           рабочая
29608      пролетариата
Name: word, dtype: object


